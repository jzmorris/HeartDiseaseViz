# -*- coding: utf-8 -*-
"""seniorproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QxS0YPw8JbbnEf--VvP7GGNwR-WzwySk
"""

!pip install interpret

from interpret import set_visualize_provider
from interpret.provider import InlineProvider
set_visualize_provider(InlineProvider())

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

from interpret.glassbox import ExplainableBoostingClassifier, LinearRegression, RegressionTree
from interpret import show
import plotly
import matplotlib.pyplot as plt

table = pd.read_csv('/content/drive/MyDrive/predictTable.csv')
seed = 1

table = table.fillna(0)
copyT = table.copy()

#one hot encode
table = pd.get_dummies(table)

#clean up data
table['percentageDeaths'] = round(table['percentageDeaths'], 2)
table['Publication Count'] = table['Publication Count'].astype(int)
table['Sum of Total Cost'] = table['Sum of Total Cost'].astype(int)
table['Project Count'] = table['Project Count'].astype(int)
#table['AVG Lead'] = round(table['AVG Lead'],2) * 100
#table['AVG Lead'] = table['AVG Lead'].astype(int)
table['combinedBadDays'] = table['combinedBadDays'].astype(int)
table['percentageDeaths'] = table['percentageDeaths'] * 100
#table['percentageDeaths'] = table['percentageDeaths'].astype(int)

train_cols = np.concatenate((table.columns[0:8],table.columns[9:])) 
label = table.columns[8]

X = table[train_cols]
y = table[label]

#80/20 split 2015-2018 and 2019
X_train = X[:232]
y_train = y[:232]

X_test = X[232:]
y_test = y[232:]

#create feature importance table

featuresList = ['Year', 'Publication Count', 'Total Projects Cost', 
'Project Count', 'Average Lead Content', 'Bad Air Days', 'Death Count (heart disease)', 'Death Count (all causes)']

featureTable = pd.DataFrame(index=featuresList)
featureTable = featureTable.reset_index()

#Linear Regression
#create future dataframe for all counties from 2020-2022
from sklearn.linear_model import LinearRegression as lrSK
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import accuracy_score
from sklearn.pipeline import make_pipeline
import math

tC = table.columns[0:8]
ll = table.columns[8]
pd.set_option('mode.chained_assignment', None)

futureTable = pd.read_csv('/content/drive/MyDrive/futureTable.csv',names= ['County', 'Year'] )

N2 = len(futureTable) / 4
columnList = table.columns[1:8]

futureTable = futureTable.sort_values(['County', 'Year'])

#single county regression 
copyT2 = copyT.sort_values(['County', 'Year'])
N = len(X) / 5

#create prediction for attributes per county 
#build out future table using per county per column linear regression 
newFuture = pd.DataFrame()

countyList = futureTable['County'].unique()

for county in countyList:
  view = copyT2[copyT2['County'] == county]

  X_train2 = copyT['Year'].unique()
  X_train2 = X_train2.reshape(-1, 1) 
  X_test2 = futureTable['Year'].unique()
  X_test2 = X_test2.reshape(-1, 1)

  midFrame = pd.DataFrame()
  midFrame['Year'] = futureTable['Year'].unique()
  midFrame['County'] = county

  for column in columnList:
    yy = view[column]

    poly = PolynomialFeatures(degree=1, include_bias=True)
    reg = lrSK()
    model = make_pipeline(poly, reg)
    model.fit(X_train2, yy)
    midFrame[column] = abs(model.predict(X_test2))

  
  newFuture = pd.concat([newFuture, midFrame])

#display(newFuture)
futureTable = newFuture

futureTable = futureTable.reset_index(drop=True)
lrColumn = []

#compute % death per county using futureTable
splits = np.array_split(copyT2, N)
splits2 = np.array_split(futureTable, N2)
lrFeatures = []
avg = 0
avgLen = len(splits)
for c in range(0, len(splits)):
  for i in range(1, 2):
    poly = PolynomialFeatures(degree=i, include_bias=True)
    reg = lrSK()
    model = make_pipeline(poly, reg)
    model.fit(splits[c][tC][:4], splits[c][ll][:4])
    #display(model.score(copyT2[tC][4:], copyT2[ll][4:]))
    predictions = abs((model.predict(splits[c][tC][4:]) - splits[c][ll][4:]) / (model.predict(splits[c][tC][4:]) + splits[c][ll][4:]) / 2) * 100
    accuracy = 100 - predictions.mean()
    if math.isnan(accuracy) != True:
      avg += accuracy
    
    else:
      avgLen -= 1
    #print("Accuracy: " )
    #print(accuracy)
    lrFeatures.append(reg.coef_[:8])
    lrColumn.append(model.predict(splits2[c][tC]))

display("accuracy average: " +str(avg / avgLen))
lrFeatures = pd.DataFrame(lrFeatures)
lrFeatures = lrFeatures.mean()
#plt.barh(featuresList, lrFeatures)
#plt.show()

featureTable['lr'] = lrFeatures

lrColumn = [item for sublist in lrColumn for item in sublist]

lrColumn = pd.Series(lrColumn).apply(lambda x: 0 if x < 0 else x)
lrColumn = lrColumn * 100
display(lrColumn)
futureTable['lrColumn'] = lrColumn.to_frame('lrColumn')

display(futureTable)

#run EBM on table
predictMerge = futureTable.copy()

ebmTable = futureTable.copy()
ebmTable = ebmTable.drop('lrColumn',axis=1)
ebmOHE = pd.get_dummies(ebmTable)

ebm = ExplainableBoostingClassifier(random_state=seed)

ebm.fit(X_train, y_train)

ebm_global = ebm.explain_global()
show(ebm_global)

ebm_local = ebm.explain_local(X_test[:5], y_test[:5])
show(ebm_local)

predictions = abs((ebm.predict(X_test) - y_test) / (ebm.predict(X_test) + y_test) / 2) * 100
# (1−uv)(1 - \frac{u}{v})(1−vu​), where uuu is the residual sum of squares ((y_true - y_pred) ** 2).sum() and vvv is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). 

r2 = 1 - ( ((y_test - ebm.predict(X_test))**2).sum() / ((y_test - ebm.predict(X_test).mean())**2).sum()) 
print(r2)
#print(ebm.score(X_test,y_test))
print("AVG Difference between prediction and training set: "+ str(predictions.mean()))
print("Accuracy: "+ str(100 - predictions.mean()))

#display(type(ebm_global.visualize()))
plotly.offline.plot(ebm_global.visualize(), filename='/content/drive/MyDrive/plot.html')

#create EBMTable
ebmTable['percentageDeaths'] = ebm.predict(ebmOHE) 

ebmTable = pd.concat([ebmTable, copyT])

ebmTable.to_csv("ebmTable", index=False)

predictMerge['ebm'] = ebm.predict(ebmOHE)
featureTable['ebm'] = [0.09551, 0.01924, 0.04141, 0.03870, 0.03365, 0.07896, 0.06765, 0.10558]

#xgboost
import xgboost as xgb

xgTable = futureTable.copy()
xgTable = xgTable.drop('lrColumn',axis=1)
xgOHE = pd.get_dummies(xgTable)

lm = xgb.XGBRegressor(objective ='reg:squarederror')

model = lm.fit(X_train,y_train)

predictions = abs((lm.predict(X_test) - y_test) / (lm.predict(X_test) + y_test) / 2) * 100
r2 = 1 - ( ((y_test - lm.predict(X_test))**2).sum() / ((y_test - lm.predict(X_test).mean())**2).sum()) 
print(r2)
print(lm.score(X_test, y_test))
print("AVG Difference between prediction and training set: "+ str(predictions.mean()))
print("Accuracy: "+ str(100 - predictions.mean()))

xgb.plot_importance(lm)
plt.show()

xgTable['percentageDeaths'] = lm.predict(xgOHE) / 100

xgTable = pd.concat([xgTable, copyT])

predictMerge['xgboost'] = lm.predict(xgOHE)
featureTable['xgboost'] = [75, 3, 2, 2, 37, 21, 88, 66]

#random forests regressor

#aka multiple decision trees

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score

rfrTable = futureTable.copy()
rfrTable = rfrTable.drop('lrColumn',axis=1)
rfrOHE = pd.get_dummies(rfrTable)

rfr = RandomForestRegressor(criterion = 'squared_error', random_state = 42)
rfr.fit(X_train, y_train)
display(rfr.score(X_train, y_train))
display(rfr.score(X_test, y_test))
plt.barh(X_train.columns[:8],rfr.feature_importances_[:8])

plt.show()

display(rfr.feature_importances_[:8])

featureTable['rfr'] = rfr.feature_importances_[:8]
featureTable.to_excel(excel_writer="featureTable.xlsx",index=False)

scores = cross_val_score(rfr, X_train, y_train, cv=5)
display(scores)

scores = cross_val_score(rfr, X_test, y_test, cv=5)
display(scores)

predictMerge['rfr'] = rfr.predict(rfrOHE)


predictMerge.to_excel(excel_writer="predictMerge.xlsx",index=False)

#display(copyT)
copyM = copyT.copy()
copyM['lrColumn'] = copyM['percentageDeaths'] * 100
copyM['ebm'] = copyM['percentageDeaths'] * 100
copyM['xgboost'] = copyM['percentageDeaths'] * 100
copyM['rfr'] = copyM['percentageDeaths'] * 100
copyM = copyM.drop('percentageDeaths', axis=1)

predictMerge = predictMerge.merge(copyM, how='outer')
predictMerge.to_excel(excel_writer="predictMerge1127.xlsx",index=False)